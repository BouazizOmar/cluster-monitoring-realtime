{
 "cells": [
  {
   "cell_type": "code",
   "id": "da29bdc5-f971-4c7d-b385-330ffd868e3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T16:05:06.591157Z",
     "start_time": "2025-04-21T16:05:06.588434Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"SPARK_HOME\"]=\"/Users/omar/Desktop/Spark\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = (\"jupyter\"\n",
    "                                       \"\")\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"] = \"lab\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T16:05:08.896138Z",
     "start_time": "2025-04-21T16:05:06.630752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(spark.version)\n",
    "print(spark.sparkContext._jvm.scala.util.Properties.versionString())"
   ],
   "id": "5e0bf8fa10b36ff6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/21 17:05:07 WARN Utils: Your hostname, MacBook-Pro-3.local resolves to a loopback address: 127.0.0.1; using 10.71.0.59 instead (on interface en0)\n",
      "25/04/21 17:05:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/21 17:05:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.5\n",
      "version 2.12.18\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:07:35.877138Z",
     "start_time": "2025-04-21T11:07:35.614911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"KafkaSparkDemo\")\n",
    "         .master(\"local[*]\")\n",
    "         .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.5\")\n",
    "         .getOrCreate())\n"
   ],
   "id": "fa2592c73022adc1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: Could not find or load main class org.apache.spark.launcher.Main\n",
      "Caused by: java.lang.ClassNotFoundException: org.apache.spark.launcher.Main\n",
      "/Users/omar/Desktop/Spark_Tut/.pyspark-env/lib/python3.13/site-packages/pyspark/bin/spark-class: line 97: CMD: bad array subscript\n",
      "head: illegal line count -- -1\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mPySparkRuntimeError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpyspark\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01msql\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SparkSession\n\u001B[32m      3\u001B[39m spark = (\u001B[43mSparkSession\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbuilder\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m         \u001B[49m\u001B[43m.\u001B[49m\u001B[43mappName\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mKafkaSparkDemo\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m         \u001B[49m\u001B[43m.\u001B[49m\u001B[43mmaster\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mlocal[*]\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m         \u001B[49m\u001B[43m.\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mspark.jars.packages\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43morg.apache.spark:spark-sql-kafka-0-10_2.12:3.5.5\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m \u001B[43m         \u001B[49m\u001B[43m.\u001B[49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Spark_Tut/.pyspark-env/lib/python3.13/site-packages/pyspark/sql/session.py:497\u001B[39m, in \u001B[36mSparkSession.Builder.getOrCreate\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    495\u001B[39m     sparkConf.set(key, value)\n\u001B[32m    496\u001B[39m \u001B[38;5;66;03m# This SparkContext may be an existing one.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m497\u001B[39m sc = \u001B[43mSparkContext\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msparkConf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    498\u001B[39m \u001B[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001B[39;00m\n\u001B[32m    499\u001B[39m \u001B[38;5;66;03m# by all sessions.\u001B[39;00m\n\u001B[32m    500\u001B[39m session = SparkSession(sc, options=\u001B[38;5;28mself\u001B[39m._options)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Spark_Tut/.pyspark-env/lib/python3.13/site-packages/pyspark/context.py:515\u001B[39m, in \u001B[36mSparkContext.getOrCreate\u001B[39m\u001B[34m(cls, conf)\u001B[39m\n\u001B[32m    513\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m SparkContext._lock:\n\u001B[32m    514\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m SparkContext._active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m515\u001B[39m         \u001B[43mSparkContext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconf\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconf\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mSparkConf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    516\u001B[39m     \u001B[38;5;28;01massert\u001B[39;00m SparkContext._active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    517\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m SparkContext._active_spark_context\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Spark_Tut/.pyspark-env/lib/python3.13/site-packages/pyspark/context.py:201\u001B[39m, in \u001B[36mSparkContext.__init__\u001B[39m\u001B[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001B[39m\n\u001B[32m    195\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m gateway \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m gateway.gateway_parameters.auth_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    196\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    197\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    198\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m is not allowed as it is a security risk.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    199\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m201\u001B[39m \u001B[43mSparkContext\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_ensure_initialized\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgateway\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgateway\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconf\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    202\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    203\u001B[39m     \u001B[38;5;28mself\u001B[39m._do_init(\n\u001B[32m    204\u001B[39m         master,\n\u001B[32m    205\u001B[39m         appName,\n\u001B[32m   (...)\u001B[39m\u001B[32m    215\u001B[39m         memory_profiler_cls,\n\u001B[32m    216\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Spark_Tut/.pyspark-env/lib/python3.13/site-packages/pyspark/context.py:436\u001B[39m, in \u001B[36mSparkContext._ensure_initialized\u001B[39m\u001B[34m(cls, instance, gateway, conf)\u001B[39m\n\u001B[32m    434\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m SparkContext._lock:\n\u001B[32m    435\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m SparkContext._gateway:\n\u001B[32m--> \u001B[39m\u001B[32m436\u001B[39m         SparkContext._gateway = gateway \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mlaunch_gateway\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    437\u001B[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001B[32m    439\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m instance:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Spark_Tut/.pyspark-env/lib/python3.13/site-packages/pyspark/java_gateway.py:107\u001B[39m, in \u001B[36mlaunch_gateway\u001B[39m\u001B[34m(conf, popen_kwargs)\u001B[39m\n\u001B[32m    104\u001B[39m     time.sleep(\u001B[32m0.1\u001B[39m)\n\u001B[32m    106\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os.path.isfile(conn_info_file):\n\u001B[32m--> \u001B[39m\u001B[32m107\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkRuntimeError(\n\u001B[32m    108\u001B[39m         error_class=\u001B[33m\"\u001B[39m\u001B[33mJAVA_GATEWAY_EXITED\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    109\u001B[39m         message_parameters={},\n\u001B[32m    110\u001B[39m     )\n\u001B[32m    112\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(conn_info_file, \u001B[33m\"\u001B[39m\u001B[33mrb\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m info:\n\u001B[32m    113\u001B[39m     gateway_port = read_int(info)\n",
      "\u001B[31mPySparkRuntimeError\u001B[39m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:07:21.971553Z",
     "start_time": "2025-04-21T11:07:21.966Z"
    }
   },
   "cell_type": "code",
   "source": "print(spark.version)",
   "id": "e3bc4c3afafa3739",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.5\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:07:30.548859Z",
     "start_time": "2025-04-21T11:07:30.218977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"KafkaSparkDemo\")\n",
    "         .master(\"local[*]\")\n",
    "         .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# Use the new PLAINTEXT_HOST listener port (29092)\n",
    "kafka_df = (spark.readStream\n",
    "      .format(\"kafka\")\n",
    "      .option(\"kafka.bootstrap.servers\", \"localhost:29092\")  # Changed from 7092 to 29092\n",
    "      .option(\"subscribe\", \"prometheus_metrics\")  # Using your existing topic\n",
    "      .option(\"startingOffsets\", \"earliest\")\n",
    "      .load())\n",
    "\n",
    "# Process the data\n",
    "parsed_df = kafka_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "b4b659808a239a60",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: Could not find or load main class org.apache.spark.launcher.Main\n",
      "Caused by: java.lang.ClassNotFoundException: org.apache.spark.launcher.Main\n",
      "/Users/omar/Desktop/Spark_Tut/.pyspark-env/lib/python3.13/site-packages/pyspark/bin/spark-class: line 97: CMD: bad array subscript\n",
      "head: illegal line count -- -1\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mPySparkRuntimeError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpyspark\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01msql\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SparkSession\n\u001B[32m      3\u001B[39m spark = (\u001B[43mSparkSession\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbuilder\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m         \u001B[49m\u001B[43m.\u001B[49m\u001B[43mappName\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mKafkaSparkDemo\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m         \u001B[49m\u001B[43m.\u001B[49m\u001B[43mmaster\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mlocal[*]\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m         \u001B[49m\u001B[43m.\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mspark.jars.packages\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43morg.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m \u001B[43m         \u001B[49m\u001B[43m.\u001B[49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m      9\u001B[39m \u001B[38;5;66;03m# Use the new PLAINTEXT_HOST listener port (29092)\u001B[39;00m\n\u001B[32m     10\u001B[39m kafka_df = (spark.readStream\n\u001B[32m     11\u001B[39m       .format(\u001B[33m\"\u001B[39m\u001B[33mkafka\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     12\u001B[39m       .option(\u001B[33m\"\u001B[39m\u001B[33mkafka.bootstrap.servers\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mlocalhost:29092\u001B[39m\u001B[33m\"\u001B[39m)  \u001B[38;5;66;03m# Changed from 7092 to 29092\u001B[39;00m\n\u001B[32m     13\u001B[39m       .option(\u001B[33m\"\u001B[39m\u001B[33msubscribe\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mprometheus_metrics\u001B[39m\u001B[33m\"\u001B[39m)  \u001B[38;5;66;03m# Using your existing topic\u001B[39;00m\n\u001B[32m     14\u001B[39m       .option(\u001B[33m\"\u001B[39m\u001B[33mstartingOffsets\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mearliest\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     15\u001B[39m       .load())\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Spark_Tut/.pyspark-env/lib/python3.13/site-packages/pyspark/sql/session.py:497\u001B[39m, in \u001B[36mSparkSession.Builder.getOrCreate\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    495\u001B[39m     sparkConf.set(key, value)\n\u001B[32m    496\u001B[39m \u001B[38;5;66;03m# This SparkContext may be an existing one.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m497\u001B[39m sc = \u001B[43mSparkContext\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msparkConf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    498\u001B[39m \u001B[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001B[39;00m\n\u001B[32m    499\u001B[39m \u001B[38;5;66;03m# by all sessions.\u001B[39;00m\n\u001B[32m    500\u001B[39m session = SparkSession(sc, options=\u001B[38;5;28mself\u001B[39m._options)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Spark_Tut/.pyspark-env/lib/python3.13/site-packages/pyspark/context.py:515\u001B[39m, in \u001B[36mSparkContext.getOrCreate\u001B[39m\u001B[34m(cls, conf)\u001B[39m\n\u001B[32m    513\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m SparkContext._lock:\n\u001B[32m    514\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m SparkContext._active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m515\u001B[39m         \u001B[43mSparkContext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconf\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconf\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mSparkConf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    516\u001B[39m     \u001B[38;5;28;01massert\u001B[39;00m SparkContext._active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    517\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m SparkContext._active_spark_context\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Spark_Tut/.pyspark-env/lib/python3.13/site-packages/pyspark/context.py:201\u001B[39m, in \u001B[36mSparkContext.__init__\u001B[39m\u001B[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001B[39m\n\u001B[32m    195\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m gateway \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m gateway.gateway_parameters.auth_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    196\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    197\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    198\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m is not allowed as it is a security risk.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    199\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m201\u001B[39m \u001B[43mSparkContext\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_ensure_initialized\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgateway\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgateway\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconf\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    202\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    203\u001B[39m     \u001B[38;5;28mself\u001B[39m._do_init(\n\u001B[32m    204\u001B[39m         master,\n\u001B[32m    205\u001B[39m         appName,\n\u001B[32m   (...)\u001B[39m\u001B[32m    215\u001B[39m         memory_profiler_cls,\n\u001B[32m    216\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Spark_Tut/.pyspark-env/lib/python3.13/site-packages/pyspark/context.py:436\u001B[39m, in \u001B[36mSparkContext._ensure_initialized\u001B[39m\u001B[34m(cls, instance, gateway, conf)\u001B[39m\n\u001B[32m    434\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m SparkContext._lock:\n\u001B[32m    435\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m SparkContext._gateway:\n\u001B[32m--> \u001B[39m\u001B[32m436\u001B[39m         SparkContext._gateway = gateway \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mlaunch_gateway\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    437\u001B[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001B[32m    439\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m instance:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Spark_Tut/.pyspark-env/lib/python3.13/site-packages/pyspark/java_gateway.py:107\u001B[39m, in \u001B[36mlaunch_gateway\u001B[39m\u001B[34m(conf, popen_kwargs)\u001B[39m\n\u001B[32m    104\u001B[39m     time.sleep(\u001B[32m0.1\u001B[39m)\n\u001B[32m    106\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os.path.isfile(conn_info_file):\n\u001B[32m--> \u001B[39m\u001B[32m107\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkRuntimeError(\n\u001B[32m    108\u001B[39m         error_class=\u001B[33m\"\u001B[39m\u001B[33mJAVA_GATEWAY_EXITED\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    109\u001B[39m         message_parameters={},\n\u001B[32m    110\u001B[39m     )\n\u001B[32m    112\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(conn_info_file, \u001B[33m\"\u001B[39m\u001B[33mrb\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m info:\n\u001B[32m    113\u001B[39m     gateway_port = read_int(info)\n",
      "\u001B[31mPySparkRuntimeError\u001B[39m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:04:13.015080Z",
     "start_time": "2025-04-21T11:04:13.012103Z"
    }
   },
   "cell_type": "code",
   "source": "parsed_df.printSchema()",
   "id": "9afcc3cdfc5925e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:00:42.362443Z",
     "start_time": "2025-04-21T11:00:42.358175Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9cebd1fbf56efac7",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:04:33.782057Z",
     "start_time": "2025-04-21T11:04:33.721912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col, from_json\n",
    "\n",
    "labels_schema = StructType([\n",
    "    StructField(\"__name__\", StringType(), True),\n",
    "    StructField(\"instance\", StringType(), True),\n",
    "    StructField(\"job\", StringType(), True),\n",
    "    StructField(\"os_type\", StringType(), True),\n",
    "    StructField(\"vm\", StringType(), True)\n",
    "])\n",
    "\n",
    "json_schema = StructType([\n",
    "    StructField(\"labels\", labels_schema, True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"value\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Kafka `value` is in binary, so cast to string first\n",
    "parsed_df = kafka_df.selectExpr(\"CAST(value AS STRING) as json_str\")\n",
    "\n",
    "# Parse JSON\n",
    "json_df = parsed_df.select(from_json(col(\"json_str\"), json_schema).alias(\"data\"))\n",
    "\n",
    "# Flatten nested fields\n",
    "flattened_df = json_df.select(\n",
    "    col(\"data.name\"),\n",
    "    col(\"data.timestamp\"),\n",
    "    col(\"data.value\"),\n",
    "    col(\"data.labels.__name__\").alias(\"metric_name\"),\n",
    "    col(\"data.labels.instance\"),\n",
    "    col(\"data.labels.job\"),\n",
    "    col(\"data.labels.os_type\"),\n",
    "    col(\"data.labels.vm\")\n",
    ")\n",
    "query = flattened_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ],
   "id": "cf8fad3055571365",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 781\n",
      "-------------------------------------------\n",
      "+----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|key |value                                                                                                                                                                                                                                            |\n",
      "+----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_guest_seconds_total\",\"cpu\":\"0\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"nice\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_guest_seconds_total\",\"timestamp\":\"2025-04-21T11:04:28Z\",\"value\":\"0\"}|\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_guest_seconds_total\",\"cpu\":\"0\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"user\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_guest_seconds_total\",\"timestamp\":\"2025-04-21T11:04:28Z\",\"value\":\"0\"}|\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_guest_seconds_total\",\"cpu\":\"1\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"nice\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_guest_seconds_total\",\"timestamp\":\"2025-04-21T11:04:28Z\",\"value\":\"0\"}|\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_guest_seconds_total\",\"cpu\":\"1\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"user\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_guest_seconds_total\",\"timestamp\":\"2025-04-21T11:04:28Z\",\"value\":\"0\"}|\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_guest_seconds_total\",\"cpu\":\"2\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"nice\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_guest_seconds_total\",\"timestamp\":\"2025-04-21T11:04:28Z\",\"value\":\"0\"}|\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_guest_seconds_total\",\"cpu\":\"2\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"user\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_guest_seconds_total\",\"timestamp\":\"2025-04-21T11:04:28Z\",\"value\":\"0\"}|\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_seconds_total\",\"cpu\":\"0\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"idle\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_seconds_total\",\"timestamp\":\"2025-04-21T11:04:28Z\",\"value\":\"3516.26\"}      |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_seconds_total\",\"cpu\":\"0\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"iowait\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_seconds_total\",\"timestamp\":\"2025-04-21T11:04:28Z\",\"value\":\"2.3\"}        |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_seconds_total\",\"cpu\":\"0\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"irq\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_seconds_total\",\"timestamp\":\"2025-04-21T11:04:28Z\",\"value\":\"0\"}             |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_seconds_total\",\"cpu\":\"0\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"nice\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_seconds_total\",\"timestamp\":\"2025-04-21T11:04:28Z\",\"value\":\"277.57\"}       |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_seconds_total\",\"cpu\":\"0\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"softirq\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_seconds_total\",\"timestamp\":\"2025-04-21T11:04:28Z\",\"value\":\"1.45\"}      |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_seconds_total\",\"cpu\":\"0\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"steal\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_seconds_total\",\"timestamp\":\"2025-04-21T11:04:28Z\",\"value\":\"0\"}           |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_seconds_total\",\"cpu\":\"0\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"system\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_seconds_total\",\"timestamp\":\"2025-04-21T11:04:28Z\",\"value\":\"83.93\"}      |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_seconds_total\",\"cpu\":\"0\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"user\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_seconds_total\",\"timestamp\":\"2025-04-21T11:04:28Z\",\"value\":\"103.73\"}       |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_seconds_total\",\"cpu\":\"1\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"idle\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_seconds_total\",\"timestamp\":\"2025-04-21T11:04:28Z\",\"value\":\"3513.97\"}      |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_seconds_total\",\"cpu\":\"1\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"iowait\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_seconds_total\",\"timestamp\":\"2025-04-21T11:04:28Z\",\"value\":\"2.43\"}       |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_seconds_total\",\"cpu\":\"1\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"irq\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_seconds_total\",\"timestamp\":\"2025-04-21T11:04:28Z\",\"value\":\"0\"}             |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_seconds_total\",\"cpu\":\"1\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"nice\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_seconds_total\",\"timestamp\":\"2025-04-21T11:04:28Z\",\"value\":\"277.87\"}       |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_seconds_total\",\"cpu\":\"1\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"softirq\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_seconds_total\",\"timestamp\":\"2025-04-21T11:04:28Z\",\"value\":\"1.09\"}      |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_seconds_total\",\"cpu\":\"1\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"steal\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_seconds_total\",\"timestamp\":\"2025-04-21T11:04:28Z\",\"value\":\"0\"}           |\n",
      "+----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:05:09.228801Z",
     "start_time": "2025-04-21T11:04:56.981236Z"
    }
   },
   "cell_type": "code",
   "source": "\n",
   "id": "e0f37fb3c314b913",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/21 12:04:56 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/w9/ddv5pgfx77ndmgb8xq_y1lbc0000gp/T/temporary-58ff5313-6e9f-4b21-abd5-6eb0aa81b2ab. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/04/21 12:04:56 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/04/21 12:04:57 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 786\n",
      "-------------------------------------------\n",
      "+----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|key |value                                                                                                                                                                                                                                   |\n",
      "+----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_memory_MemFree_bytes\",\"instance\":\"10.71.0.59:9101\",\"job\":\"node\",\"os_type\":\"linux\",\"vm\":\"Lubuntu\"},\"name\":\"node_memory_MemFree_bytes\",\"timestamp\":\"2025-04-21T11:04:53Z\",\"value\":\"2277191680\"}               |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_memory_MemTotal_bytes\",\"instance\":\"10.71.0.59:9101\",\"job\":\"node\",\"os_type\":\"linux\",\"vm\":\"Lubuntu\"},\"name\":\"node_memory_MemTotal_bytes\",\"timestamp\":\"2025-04-21T11:04:53Z\",\"value\":\"3668586496\"}             |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_memory_Mlocked_bytes\",\"instance\":\"10.71.0.59:9101\",\"job\":\"node\",\"os_type\":\"linux\",\"vm\":\"Lubuntu\"},\"name\":\"node_memory_Mlocked_bytes\",\"timestamp\":\"2025-04-21T11:04:53Z\",\"value\":\"25866240\"}                 |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_memory_NFS_Unstable_bytes\",\"instance\":\"10.71.0.59:9101\",\"job\":\"node\",\"os_type\":\"linux\",\"vm\":\"Lubuntu\"},\"name\":\"node_memory_NFS_Unstable_bytes\",\"timestamp\":\"2025-04-21T11:04:53Z\",\"value\":\"0\"}              |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_memory_PageTables_bytes\",\"instance\":\"10.71.0.59:9101\",\"job\":\"node\",\"os_type\":\"linux\",\"vm\":\"Lubuntu\"},\"name\":\"node_memory_PageTables_bytes\",\"timestamp\":\"2025-04-21T11:04:53Z\",\"value\":\"13733888\"}           |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_memory_Percpu_bytes\",\"instance\":\"10.71.0.59:9101\",\"job\":\"node\",\"os_type\":\"linux\",\"vm\":\"Lubuntu\"},\"name\":\"node_memory_Percpu_bytes\",\"timestamp\":\"2025-04-21T11:04:53Z\",\"value\":\"1351680\"}                    |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_memory_SReclaimable_bytes\",\"instance\":\"10.71.0.59:9101\",\"job\":\"node\",\"os_type\":\"linux\",\"vm\":\"Lubuntu\"},\"name\":\"node_memory_SReclaimable_bytes\",\"timestamp\":\"2025-04-21T11:04:53Z\",\"value\":\"35405824\"}       |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_memory_SUnreclaim_bytes\",\"instance\":\"10.71.0.59:9101\",\"job\":\"node\",\"os_type\":\"linux\",\"vm\":\"Lubuntu\"},\"name\":\"node_memory_SUnreclaim_bytes\",\"timestamp\":\"2025-04-21T11:04:53Z\",\"value\":\"85405696\"}           |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_memory_SecPageTables_bytes\",\"instance\":\"10.71.0.59:9101\",\"job\":\"node\",\"os_type\":\"linux\",\"vm\":\"Lubuntu\"},\"name\":\"node_memory_SecPageTables_bytes\",\"timestamp\":\"2025-04-21T11:04:53Z\",\"value\":\"0\"}            |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_memory_ShadowCallStack_bytes\",\"instance\":\"10.71.0.59:9101\",\"job\":\"node\",\"os_type\":\"linux\",\"vm\":\"Lubuntu\"},\"name\":\"node_memory_ShadowCallStack_bytes\",\"timestamp\":\"2025-04-21T11:04:53Z\",\"value\":\"1601536\"}  |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_memory_ShmemHugePages_bytes\",\"instance\":\"10.71.0.59:9101\",\"job\":\"node\",\"os_type\":\"linux\",\"vm\":\"Lubuntu\"},\"name\":\"node_memory_ShmemHugePages_bytes\",\"timestamp\":\"2025-04-21T11:04:53Z\",\"value\":\"0\"}          |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_memory_ShmemPmdMapped_bytes\",\"instance\":\"10.71.0.59:9101\",\"job\":\"node\",\"os_type\":\"linux\",\"vm\":\"Lubuntu\"},\"name\":\"node_memory_ShmemPmdMapped_bytes\",\"timestamp\":\"2025-04-21T11:04:53Z\",\"value\":\"0\"}          |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_memory_Shmem_bytes\",\"instance\":\"10.71.0.59:9101\",\"job\":\"node\",\"os_type\":\"linux\",\"vm\":\"Lubuntu\"},\"name\":\"node_memory_Shmem_bytes\",\"timestamp\":\"2025-04-21T11:04:53Z\",\"value\":\"11108352\"}                     |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_memory_Slab_bytes\",\"instance\":\"10.71.0.59:9101\",\"job\":\"node\",\"os_type\":\"linux\",\"vm\":\"Lubuntu\"},\"name\":\"node_memory_Slab_bytes\",\"timestamp\":\"2025-04-21T11:04:53Z\",\"value\":\"120811520\"}                      |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_memory_SwapCached_bytes\",\"instance\":\"10.71.0.59:9101\",\"job\":\"node\",\"os_type\":\"linux\",\"vm\":\"Lubuntu\"},\"name\":\"node_memory_SwapCached_bytes\",\"timestamp\":\"2025-04-21T11:04:53Z\",\"value\":\"0\"}                  |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_memory_SwapFree_bytes\",\"instance\":\"10.71.0.59:9101\",\"job\":\"node\",\"os_type\":\"linux\",\"vm\":\"Lubuntu\"},\"name\":\"node_memory_SwapFree_bytes\",\"timestamp\":\"2025-04-21T11:04:53Z\",\"value\":\"2125459456\"}             |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_memory_SwapTotal_bytes\",\"instance\":\"10.71.0.59:9101\",\"job\":\"node\",\"os_type\":\"linux\",\"vm\":\"Lubuntu\"},\"name\":\"node_memory_SwapTotal_bytes\",\"timestamp\":\"2025-04-21T11:04:53Z\",\"value\":\"2125459456\"}           |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_memory_Unevictable_bytes\",\"instance\":\"10.71.0.59:9101\",\"job\":\"node\",\"os_type\":\"linux\",\"vm\":\"Lubuntu\"},\"name\":\"node_memory_Unevictable_bytes\",\"timestamp\":\"2025-04-21T11:04:53Z\",\"value\":\"25866240\"}         |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_memory_VmallocChunk_bytes\",\"instance\":\"10.71.0.59:9101\",\"job\":\"node\",\"os_type\":\"linux\",\"vm\":\"Lubuntu\"},\"name\":\"node_memory_VmallocChunk_bytes\",\"timestamp\":\"2025-04-21T11:04:53Z\",\"value\":\"0\"}              |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_memory_VmallocTotal_bytes\",\"instance\":\"10.71.0.59:9101\",\"job\":\"node\",\"os_type\":\"linux\",\"vm\":\"Lubuntu\"},\"name\":\"node_memory_VmallocTotal_bytes\",\"timestamp\":\"2025-04-21T11:04:53Z\",\"value\":\"138535235485696\"}|\n",
      "+----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 787\n",
      "-------------------------------------------\n",
      "+----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|key |value                                                                                                                                                                                                                                            |\n",
      "+----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_guest_seconds_total\",\"cpu\":\"0\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"nice\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_guest_seconds_total\",\"timestamp\":\"2025-04-21T11:04:58Z\",\"value\":\"0\"}|\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_guest_seconds_total\",\"cpu\":\"0\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"user\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_guest_seconds_total\",\"timestamp\":\"2025-04-21T11:04:58Z\",\"value\":\"0\"}|\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_guest_seconds_total\",\"cpu\":\"1\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"nice\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_guest_seconds_total\",\"timestamp\":\"2025-04-21T11:04:58Z\",\"value\":\"0\"}|\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_guest_seconds_total\",\"cpu\":\"1\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"user\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_guest_seconds_total\",\"timestamp\":\"2025-04-21T11:04:58Z\",\"value\":\"0\"}|\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_guest_seconds_total\",\"cpu\":\"2\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"nice\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_guest_seconds_total\",\"timestamp\":\"2025-04-21T11:04:58Z\",\"value\":\"0\"}|\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_guest_seconds_total\",\"cpu\":\"2\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"user\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_guest_seconds_total\",\"timestamp\":\"2025-04-21T11:04:58Z\",\"value\":\"0\"}|\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_seconds_total\",\"cpu\":\"0\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"idle\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_seconds_total\",\"timestamp\":\"2025-04-21T11:04:58Z\",\"value\":\"3541.71\"}      |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_seconds_total\",\"cpu\":\"0\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"iowait\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_seconds_total\",\"timestamp\":\"2025-04-21T11:04:58Z\",\"value\":\"2.3\"}        |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_seconds_total\",\"cpu\":\"0\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"irq\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_seconds_total\",\"timestamp\":\"2025-04-21T11:04:58Z\",\"value\":\"0\"}             |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_seconds_total\",\"cpu\":\"0\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"nice\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_seconds_total\",\"timestamp\":\"2025-04-21T11:04:58Z\",\"value\":\"279.8\"}        |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_seconds_total\",\"cpu\":\"0\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"softirq\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_seconds_total\",\"timestamp\":\"2025-04-21T11:04:58Z\",\"value\":\"1.46\"}      |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_seconds_total\",\"cpu\":\"0\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"steal\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_seconds_total\",\"timestamp\":\"2025-04-21T11:04:58Z\",\"value\":\"0\"}           |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_seconds_total\",\"cpu\":\"0\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"system\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_seconds_total\",\"timestamp\":\"2025-04-21T11:04:58Z\",\"value\":\"84.53\"}      |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_seconds_total\",\"cpu\":\"0\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"user\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_seconds_total\",\"timestamp\":\"2025-04-21T11:04:58Z\",\"value\":\"104.36\"}       |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_seconds_total\",\"cpu\":\"1\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"idle\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_seconds_total\",\"timestamp\":\"2025-04-21T11:04:58Z\",\"value\":\"3539.26\"}      |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_seconds_total\",\"cpu\":\"1\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"iowait\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_seconds_total\",\"timestamp\":\"2025-04-21T11:04:58Z\",\"value\":\"2.43\"}       |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_seconds_total\",\"cpu\":\"1\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"irq\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_seconds_total\",\"timestamp\":\"2025-04-21T11:04:58Z\",\"value\":\"0\"}             |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_seconds_total\",\"cpu\":\"1\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"nice\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_seconds_total\",\"timestamp\":\"2025-04-21T11:04:58Z\",\"value\":\"280.04\"}       |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_seconds_total\",\"cpu\":\"1\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"softirq\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_seconds_total\",\"timestamp\":\"2025-04-21T11:04:58Z\",\"value\":\"1.09\"}      |\n",
      "|NULL|{\"labels\":{\"__name__\":\"node_cpu_seconds_total\",\"cpu\":\"1\",\"instance\":\"10.71.0.59:9103\",\"job\":\"node\",\"mode\":\"steal\",\"os_type\":\"linux\",\"vm\":\"Lubuntu V2\"},\"name\":\"node_cpu_seconds_total\",\"timestamp\":\"2025-04-21T11:04:58Z\",\"value\":\"0\"}           |\n",
      "+----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/21 12:05:08 ERROR Executor: Exception in task 0.0 in stage 786.0 (TID 786)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:71)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:391)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1(SerializerHelper.scala:40)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1$adapted(SerializerHelper.scala:40)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$$$Lambda/0x000000b001e5b6e8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1875)\n",
      "\tat java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:725)\n",
      "\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1(Utils.scala:209)\n",
      "\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1$adapted(Utils.scala:209)\n",
      "\tat org.apache.spark.util.Utils$$$Lambda/0x000000b001e66758.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.writeByteBufferImpl(Utils.scala:187)\n",
      "\tat org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:209)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2(ChunkedByteBuffer.scala:103)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2$adapted(ChunkedByteBuffer.scala:103)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$$Lambda/0x000000b001e66388.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.writeExternal(ChunkedByteBuffer.scala:103)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$1(TaskResult.scala:60)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult$$Lambda/0x000000b001e65cd8.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n",
      "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:59)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1478)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1449)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1194)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:358)\n",
      "25/04/21 12:05:08 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[#187,Executor task launch worker for task 0.0 in stage 786.0 (TID 786),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:71)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:391)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1(SerializerHelper.scala:40)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1$adapted(SerializerHelper.scala:40)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$$$Lambda/0x000000b001e5b6e8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1875)\n",
      "\tat java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:725)\n",
      "\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1(Utils.scala:209)\n",
      "\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1$adapted(Utils.scala:209)\n",
      "\tat org.apache.spark.util.Utils$$$Lambda/0x000000b001e66758.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.writeByteBufferImpl(Utils.scala:187)\n",
      "\tat org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:209)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2(ChunkedByteBuffer.scala:103)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2$adapted(ChunkedByteBuffer.scala:103)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$$Lambda/0x000000b001e66388.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.writeExternal(ChunkedByteBuffer.scala:103)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$1(TaskResult.scala:60)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult$$Lambda/0x000000b001e65cd8.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n",
      "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:59)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1478)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1449)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1194)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:358)\n",
      "25/04/21 12:05:08 WARN TaskSetManager: Lost task 0.0 in stage 786.0 (TID 786) (10.71.0.59 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:71)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:391)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1(SerializerHelper.scala:40)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1$adapted(SerializerHelper.scala:40)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$$$Lambda/0x000000b001e5b6e8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1875)\n",
      "\tat java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:725)\n",
      "\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1(Utils.scala:209)\n",
      "\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1$adapted(Utils.scala:209)\n",
      "\tat org.apache.spark.util.Utils$$$Lambda/0x000000b001e66758.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.writeByteBufferImpl(Utils.scala:187)\n",
      "\tat org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:209)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2(ChunkedByteBuffer.scala:103)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2$adapted(ChunkedByteBuffer.scala:103)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$$Lambda/0x000000b001e66388.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.writeExternal(ChunkedByteBuffer.scala:103)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$1(TaskResult.scala:60)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult$$Lambda/0x000000b001e65cd8.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n",
      "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:59)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1478)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1449)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1194)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:358)\n",
      "\n",
      "25/04/21 12:05:08 ERROR TaskSetManager: Task 0 in stage 786.0 failed 1 times; aborting job\n",
      "25/04/21 12:05:08 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=false]] is aborting.\n",
      "25/04/21 12:05:08 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=false]] aborted.\n",
      "25/04/21 12:05:08 ERROR MicroBatchExecution: Query [id = a71f0b34-df46-4d2e-b35f-3ca732b4ab90, runId = ee4d0c0d-f317-4880-a0e3-ce4c43d36dd6] terminated with error\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 786.0 failed 1 times, most recent failure: Lost task 0.0 in stage 786.0 (TID 786) (10.71.0.59 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:71)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:391)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1(SerializerHelper.scala:40)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1$adapted(SerializerHelper.scala:40)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$$$Lambda/0x000000b001e5b6e8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1875)\n",
      "\tat java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:725)\n",
      "\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1(Utils.scala:209)\n",
      "\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1$adapted(Utils.scala:209)\n",
      "\tat org.apache.spark.util.Utils$$$Lambda/0x000000b001e66758.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.writeByteBufferImpl(Utils.scala:187)\n",
      "\tat org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:209)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2(ChunkedByteBuffer.scala:103)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2$adapted(ChunkedByteBuffer.scala:103)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$$Lambda/0x000000b001e66388.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.writeExternal(ChunkedByteBuffer.scala:103)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$1(TaskResult.scala:60)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult$$Lambda/0x000000b001e65cd8.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n",
      "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:59)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1478)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1449)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1194)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:358)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:390)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n",
      "\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:71)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:391)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1(SerializerHelper.scala:40)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1$adapted(SerializerHelper.scala:40)\n",
      "\tat org.apache.spark.serializer.SerializerHelper$$$Lambda/0x000000b001e5b6e8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1875)\n",
      "\tat java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:725)\n",
      "\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1(Utils.scala:209)\n",
      "\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1$adapted(Utils.scala:209)\n",
      "\tat org.apache.spark.util.Utils$$$Lambda/0x000000b001e66758.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.writeByteBufferImpl(Utils.scala:187)\n",
      "\tat org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:209)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2(ChunkedByteBuffer.scala:103)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2$adapted(ChunkedByteBuffer.scala:103)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer$$Lambda/0x000000b001e66388.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBuffer.writeExternal(ChunkedByteBuffer.scala:103)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$1(TaskResult.scala:60)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult$$Lambda/0x000000b001e65cd8.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n",
      "\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\n",
      "\tat org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:59)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1478)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1449)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1194)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:358)\n",
      "25/04/21 12:05:08 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 788, writer: ConsoleWriter[numRows=20, truncate=false]] is aborting.\n",
      "25/04/21 12:05:08 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 788, writer: ConsoleWriter[numRows=20, truncate=false]] aborted.\n",
      "25/04/21 12:05:08 ERROR MicroBatchExecution: Query [id = 1966a7cf-f2b6-42af-ba90-d1fdacc595ec, runId = 3dfb36c8-9f0b-4bc4-9e50-690fc368220d] terminated with error\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "         \n",
      "java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n",
      "\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1654)\n",
      "\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1585)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1402)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1337)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3003)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1595)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1402)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1337)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3003)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:390)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n",
      "\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "Caused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:75)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n",
      "\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1654)\n",
      "\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1585)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1402)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1337)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3003)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n"
     ]
    },
    {
     "ename": "StreamingQueryException",
     "evalue": "[STREAM_FAILED] Query [id = a71f0b34-df46-4d2e-b35f-3ca732b4ab90, runId = ee4d0c0d-f317-4880-a0e3-ce4c43d36dd6] terminated with exception: Job aborted due to stage failure: Task 0 in stage 786.0 failed 1 times, most recent failure: Lost task 0.0 in stage 786.0 (TID 786) (10.71.0.59 executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:71)\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:391)\n\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1(SerializerHelper.scala:40)\n\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1$adapted(SerializerHelper.scala:40)\n\tat org.apache.spark.serializer.SerializerHelper$$$Lambda/0x000000b001e5b6e8.apply(Unknown Source)\n\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1875)\n\tat java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:725)\n\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1(Utils.scala:209)\n\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1$adapted(Utils.scala:209)\n\tat org.apache.spark.util.Utils$$$Lambda/0x000000b001e66758.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.writeByteBufferImpl(Utils.scala:187)\n\tat org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:209)\n\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2(ChunkedByteBuffer.scala:103)\n\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2$adapted(ChunkedByteBuffer.scala:103)\n\tat org.apache.spark.util.io.ChunkedByteBuffer$$Lambda/0x000000b001e66388.apply(Unknown Source)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat org.apache.spark.util.io.ChunkedByteBuffer.writeExternal(ChunkedByteBuffer.scala:103)\n\tat org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$1(TaskResult.scala:60)\n\tat org.apache.spark.scheduler.DirectTaskResult$$Lambda/0x000000b001e65cd8.apply$mcV$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\n\tat org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:59)\n\tat java.base/java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1478)\n\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1449)\n\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1194)\n\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:358)\n\nDriver stacktrace:",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mStreamingQueryException\u001B[39m                   Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      1\u001B[39m query = flattened_df.writeStream \\\n\u001B[32m      2\u001B[39m     .outputMode(\u001B[33m\"\u001B[39m\u001B[33mappend\u001B[39m\u001B[33m\"\u001B[39m) \\\n\u001B[32m      3\u001B[39m     .format(\u001B[33m\"\u001B[39m\u001B[33mconsole\u001B[39m\u001B[33m\"\u001B[39m) \\\n\u001B[32m      4\u001B[39m     .option(\u001B[33m\"\u001B[39m\u001B[33mtruncate\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m) \\\n\u001B[32m      5\u001B[39m     .start()\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m \u001B[43mquery\u001B[49m\u001B[43m.\u001B[49m\u001B[43mawaitTermination\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Spark_Tut/.pyspark-env/lib/python3.13/site-packages/pyspark/sql/streaming/query.py:221\u001B[39m, in \u001B[36mStreamingQuery.awaitTermination\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    219\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._jsq.awaitTermination(\u001B[38;5;28mint\u001B[39m(timeout * \u001B[32m1000\u001B[39m))\n\u001B[32m    220\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m221\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_jsq\u001B[49m\u001B[43m.\u001B[49m\u001B[43mawaitTermination\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Spark_Tut/.pyspark-env/lib/python3.13/site-packages/py4j/java_gateway.py:1322\u001B[39m, in \u001B[36mJavaMember.__call__\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m   1316\u001B[39m command = proto.CALL_COMMAND_NAME +\\\n\u001B[32m   1317\u001B[39m     \u001B[38;5;28mself\u001B[39m.command_header +\\\n\u001B[32m   1318\u001B[39m     args_command +\\\n\u001B[32m   1319\u001B[39m     proto.END_COMMAND_PART\n\u001B[32m   1321\u001B[39m answer = \u001B[38;5;28mself\u001B[39m.gateway_client.send_command(command)\n\u001B[32m-> \u001B[39m\u001B[32m1322\u001B[39m return_value = \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1323\u001B[39m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1325\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[32m   1326\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[33m\"\u001B[39m\u001B[33m_detach\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Spark_Tut/.pyspark-env/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:185\u001B[39m, in \u001B[36mcapture_sql_exception.<locals>.deco\u001B[39m\u001B[34m(*a, **kw)\u001B[39m\n\u001B[32m    181\u001B[39m converted = convert_exception(e.java_exception)\n\u001B[32m    182\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[32m    183\u001B[39m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[32m    184\u001B[39m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m185\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    186\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    187\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[31mStreamingQueryException\u001B[39m: [STREAM_FAILED] Query [id = a71f0b34-df46-4d2e-b35f-3ca732b4ab90, runId = ee4d0c0d-f317-4880-a0e3-ce4c43d36dd6] terminated with exception: Job aborted due to stage failure: Task 0 in stage 786.0 failed 1 times, most recent failure: Lost task 0.0 in stage 786.0 (TID 786) (10.71.0.59 executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:71)\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:391)\n\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1(SerializerHelper.scala:40)\n\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1$adapted(SerializerHelper.scala:40)\n\tat org.apache.spark.serializer.SerializerHelper$$$Lambda/0x000000b001e5b6e8.apply(Unknown Source)\n\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1875)\n\tat java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:725)\n\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1(Utils.scala:209)\n\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1$adapted(Utils.scala:209)\n\tat org.apache.spark.util.Utils$$$Lambda/0x000000b001e66758.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.writeByteBufferImpl(Utils.scala:187)\n\tat org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:209)\n\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2(ChunkedByteBuffer.scala:103)\n\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2$adapted(ChunkedByteBuffer.scala:103)\n\tat org.apache.spark.util.io.ChunkedByteBuffer$$Lambda/0x000000b001e66388.apply(Unknown Source)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat org.apache.spark.util.io.ChunkedByteBuffer.writeExternal(ChunkedByteBuffer.scala:103)\n\tat org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$1(TaskResult.scala:60)\n\tat org.apache.spark.scheduler.DirectTaskResult$$Lambda/0x000000b001e65cd8.apply$mcV$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException(SparkErrorUtils.scala:35)\n\tat org.apache.spark.util.SparkErrorUtils.tryOrIOException$(SparkErrorUtils.scala:33)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:94)\n\tat org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:59)\n\tat java.base/java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1478)\n\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1449)\n\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1194)\n\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:358)\n\nDriver stacktrace:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/omar/Desktop/Spark_Tut/.pyspark-env/lib/python3.13/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/omar/Desktop/Spark_Tut/.pyspark-env/lib/python3.13/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/omar/Desktop/Spark_Tut/.pyspark-env/lib/python3.13/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "        \"Error while sending or receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "58d3e0b2b3ebc8a3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
