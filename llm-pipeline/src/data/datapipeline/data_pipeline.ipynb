{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "ExecuteTime": {
     "end_time": "2025-05-17T17:32:41.684213Z",
     "start_time": "2025-05-17T17:32:41.569422Z"
    }
   },
   "source": [
    "from minio_service import MinioService"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T17:32:44.837655Z",
     "start_time": "2025-05-17T17:32:42.223920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# configure and instantiate\n",
    "minio_svc = MinioService()\n",
    "\n",
    "# list and fetch raw JSON blobs\n",
    "objects = minio_svc.list_all_objects()\n",
    "print(f\"Found {len(objects)} objects in bucket.\")\n",
    "\n",
    "# pick one and load it\n",
    "sample_obj = objects[0].object_name\n",
    "raw = minio_svc.get_object_data(sample_obj)\n",
    "print(raw[:2], \"…\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Found 28668 objects in bucket 'warehouse'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28668 objects in bucket.\n",
      "{\" …\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T17:32:48.530483Z",
     "start_time": "2025-05-17T17:32:48.520317Z"
    }
   },
   "source": [
    "# cell 2\n",
    "from data_parser import DataParser\n",
    "\n",
    "parsed = DataParser.parse_minio_data(raw)\n",
    "print(f\"Parsed {len(parsed)} records; sample:\", parsed[0])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 1000 records; sample: {'name': 'node_systemd_unit_state', 'value': '1', 'labels': {'instance': '10.71.0.59:9102', '__name__': 'node_systemd_unit_state', 'os_type': 'linux', 'vm': 'Ubuntu', 'name': 'initrd-switch-root.service', 'state': 'inactive', 'job': 'node', 'type': 'oneshot'}, 'timestamp': datetime.datetime(2025, 4, 24, 14, 17, 18, tzinfo=<UTC>)}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T17:32:49.176244Z",
     "start_time": "2025-05-17T17:32:49.173222Z"
    }
   },
   "source": [
    "# Add after parsing data\n",
    "print(\"Value types in parsed data:\")\n",
    "value_types = {}\n",
    "for record in parsed:\n",
    "    value_type = type(record.get('value')).__name__\n",
    "    value_types[value_type] = value_types.get(value_type, 0) + 1\n",
    "print(value_types)\n",
    "\n",
    "# Check for string 'NaN'\n",
    "string_nan_count = sum(1 for record in parsed if record.get('value') == 'NaN')\n",
    "print(f\"Records with 'NaN' as string: {string_nan_count}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value types in parsed data:\n",
      "{'str': 1000}\n",
      "Records with 'NaN' as string: 0\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "ExecuteTime": {
     "end_time": "2025-05-17T17:32:54.012997Z",
     "start_time": "2025-05-17T17:32:49.989683Z"
    }
   },
   "source": [
    "# cell 3\n",
    "from metrics_processor import MetricsProcessor\n",
    "import pandas as pd\n",
    "\n",
    "# keep only your “CRITICAL_METRICS”\n",
    "critical = MetricsProcessor.filter_critical_data(parsed)\n",
    "\n",
    "# turn into list of dicts with {vm, metric, value, …}\n",
    "structured = MetricsProcessor.structure_metrics(critical)\n",
    "\n",
    "# DataFrame for downstream\n",
    "df = pd.DataFrame(structured)\n",
    "display(df.head())\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       vm                   metric  value         state  \\\n",
       "0  Ubuntu  node_systemd_unit_state    1.0      inactive   \n",
       "1  Ubuntu  node_systemd_unit_state    0.0    activating   \n",
       "2  Ubuntu  node_systemd_unit_state    0.0        active   \n",
       "3  Ubuntu  node_systemd_unit_state    0.0  deactivating   \n",
       "4  Ubuntu  node_systemd_unit_state    0.0        failed   \n",
       "\n",
       "                      service                 timestamp  \n",
       "0  initrd-switch-root.service 2025-04-24 14:17:18+00:00  \n",
       "1   initrd-switch-root.target 2025-04-24 14:17:18+00:00  \n",
       "2   initrd-switch-root.target 2025-04-24 14:17:18+00:00  \n",
       "3   initrd-switch-root.target 2025-04-24 14:17:18+00:00  \n",
       "4   initrd-switch-root.target 2025-04-24 14:17:18+00:00  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vm</th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "      <th>state</th>\n",
       "      <th>service</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ubuntu</td>\n",
       "      <td>node_systemd_unit_state</td>\n",
       "      <td>1.0</td>\n",
       "      <td>inactive</td>\n",
       "      <td>initrd-switch-root.service</td>\n",
       "      <td>2025-04-24 14:17:18+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ubuntu</td>\n",
       "      <td>node_systemd_unit_state</td>\n",
       "      <td>0.0</td>\n",
       "      <td>activating</td>\n",
       "      <td>initrd-switch-root.target</td>\n",
       "      <td>2025-04-24 14:17:18+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ubuntu</td>\n",
       "      <td>node_systemd_unit_state</td>\n",
       "      <td>0.0</td>\n",
       "      <td>active</td>\n",
       "      <td>initrd-switch-root.target</td>\n",
       "      <td>2025-04-24 14:17:18+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ubuntu</td>\n",
       "      <td>node_systemd_unit_state</td>\n",
       "      <td>0.0</td>\n",
       "      <td>deactivating</td>\n",
       "      <td>initrd-switch-root.target</td>\n",
       "      <td>2025-04-24 14:17:18+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ubuntu</td>\n",
       "      <td>node_systemd_unit_state</td>\n",
       "      <td>0.0</td>\n",
       "      <td>failed</td>\n",
       "      <td>initrd-switch-root.target</td>\n",
       "      <td>2025-04-24 14:17:18+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T17:40:43.146985Z",
     "start_time": "2025-05-17T17:40:43.012612Z"
    }
   },
   "source": [
    "# cell 4\n",
    "from snapshot_generator import SnapshotGenerator\n",
    "\n",
    "snap_gen = SnapshotGenerator()\n",
    "# group into 5-minute windows, per VM\n",
    "snapshots, per_vm_prompts, multi_vm = snap_gen.generate_prompts_from_df(df, window_minutes=5)\n",
    "\n",
    "print(f\"→ {len(snapshots)} snapshots\")\n",
    "print(\"Example prompt:\\n\", per_vm_prompts[0])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ 1 snapshots\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mIndexError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 9\u001B[39m\n\u001B[32m      6\u001B[39m snapshots, per_vm_prompts, multi_vm = snap_gen.generate_prompts_from_df(df, window_minutes=\u001B[32m5\u001B[39m)\n\u001B[32m      8\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m→ \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(snapshots)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m snapshots\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mExample prompt:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m, \u001B[43mper_vm_prompts\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m]\u001B[49m)\n",
      "\u001B[31mIndexError\u001B[39m: list index out of range"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T17:33:01.438788Z",
     "start_time": "2025-05-17T17:32:55.761303Z"
    }
   },
   "source": [
    "# cell 5\n",
    "from feature_extractor import FeatureExtractor\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# extract feature vectors\n",
    "features = np.vstack([FeatureExtractor.extract_features(s) for s in snapshots])\n",
    "\n",
    "# normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(features)\n",
    "fn = scaler.transform(features)\n",
    "\n",
    "# torch DataLoader\n",
    "tensor = torch.tensor(fn, dtype=torch.float32)\n",
    "dl = DataLoader(TensorDataset(tensor), batch_size=2, shuffle=True)\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T17:33:48.160073Z",
     "start_time": "2025-05-17T17:33:48.157497Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T17:33:02.526923Z",
     "start_time": "2025-05-17T17:33:01.445198Z"
    }
   },
   "source": [
    "# cell 6\n",
    "from anomaly_detector import AnomalyDetector\n",
    "import torch\n",
    "\n",
    "# build and train\n",
    "model = AnomalyDetector(input_dim=tensor.shape[1], latent_dim=15, dropout_rate=0.2)\n",
    "model.train_autoencoder(dl, num_epochs=300, lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "# compute threshold on the same normalized dataset\n",
    "model.compute_threshold(tensor)\n",
    "print(\"Threshold:\", model.threshold.item())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.0759\n",
      "Epoch 2: Loss = 0.0633\n",
      "Epoch 3: Loss = 0.0621\n",
      "Epoch 4: Loss = 0.0619\n",
      "Epoch 5: Loss = 0.0649\n",
      "Epoch 6: Loss = 0.0587\n",
      "Epoch 7: Loss = 0.0596\n",
      "Epoch 8: Loss = 0.0373\n",
      "Epoch 9: Loss = 0.0413\n",
      "Epoch 10: Loss = 0.0573\n",
      "Epoch 11: Loss = 0.0451\n",
      "Epoch 12: Loss = 0.0511\n",
      "Epoch 13: Loss = 0.0396\n",
      "Epoch 14: Loss = 0.0508\n",
      "Epoch 15: Loss = 0.0441\n",
      "Epoch 16: Loss = 0.0354\n",
      "Epoch 17: Loss = 0.0408\n",
      "Epoch 18: Loss = 0.0308\n",
      "Epoch 19: Loss = 0.0385\n",
      "Epoch 20: Loss = 0.0357\n",
      "Epoch 21: Loss = 0.0354\n",
      "Epoch 22: Loss = 0.0364\n",
      "Epoch 23: Loss = 0.0326\n",
      "Epoch 24: Loss = 0.0326\n",
      "Epoch 25: Loss = 0.0320\n",
      "Epoch 26: Loss = 0.0269\n",
      "Epoch 27: Loss = 0.0260\n",
      "Epoch 28: Loss = 0.0251\n",
      "Epoch 29: Loss = 0.0209\n",
      "Epoch 30: Loss = 0.0264\n",
      "Epoch 31: Loss = 0.0261\n",
      "Epoch 32: Loss = 0.0241\n",
      "Epoch 33: Loss = 0.0218\n",
      "Epoch 34: Loss = 0.0201\n",
      "Epoch 35: Loss = 0.0179\n",
      "Epoch 36: Loss = 0.0227\n",
      "Epoch 37: Loss = 0.0208\n",
      "Epoch 38: Loss = 0.0172\n",
      "Epoch 39: Loss = 0.0201\n",
      "Epoch 40: Loss = 0.0195\n",
      "Epoch 41: Loss = 0.0158\n",
      "Epoch 42: Loss = 0.0138\n",
      "Epoch 43: Loss = 0.0147\n",
      "Epoch 44: Loss = 0.0144\n",
      "Epoch 45: Loss = 0.0136\n",
      "Epoch 46: Loss = 0.0152\n",
      "Epoch 47: Loss = 0.0125\n",
      "Epoch 48: Loss = 0.0109\n",
      "Epoch 49: Loss = 0.0117\n",
      "Epoch 50: Loss = 0.0136\n",
      "Epoch 51: Loss = 0.0117\n",
      "Epoch 52: Loss = 0.0122\n",
      "Epoch 53: Loss = 0.0164\n",
      "Epoch 54: Loss = 0.0105\n",
      "Epoch 55: Loss = 0.0112\n",
      "Epoch 56: Loss = 0.0115\n",
      "Epoch 57: Loss = 0.0172\n",
      "Epoch 58: Loss = 0.0096\n",
      "Epoch 59: Loss = 0.0076\n",
      "Epoch 60: Loss = 0.0074\n",
      "Epoch 61: Loss = 0.0074\n",
      "Epoch 62: Loss = 0.0079\n",
      "Epoch 63: Loss = 0.0061\n",
      "Epoch 64: Loss = 0.0077\n",
      "Epoch 65: Loss = 0.0059\n",
      "Epoch 66: Loss = 0.0083\n",
      "Epoch 67: Loss = 0.0064\n",
      "Epoch 68: Loss = 0.0117\n",
      "Epoch 69: Loss = 0.0051\n",
      "Epoch 70: Loss = 0.0066\n",
      "Epoch 71: Loss = 0.0059\n",
      "Epoch 72: Loss = 0.0133\n",
      "Epoch 73: Loss = 0.0052\n",
      "Epoch 74: Loss = 0.0042\n",
      "Epoch 75: Loss = 0.0092\n",
      "Epoch 76: Loss = 0.0053\n",
      "Epoch 77: Loss = 0.0046\n",
      "Epoch 78: Loss = 0.0051\n",
      "Epoch 79: Loss = 0.0055\n",
      "Epoch 80: Loss = 0.0038\n",
      "Epoch 81: Loss = 0.0051\n",
      "Epoch 82: Loss = 0.0063\n",
      "Epoch 83: Loss = 0.0029\n",
      "Epoch 84: Loss = 0.0041\n",
      "Epoch 85: Loss = 0.0036\n",
      "Epoch 86: Loss = 0.0039\n",
      "Epoch 87: Loss = 0.0028\n",
      "Epoch 88: Loss = 0.0061\n",
      "Epoch 89: Loss = 0.0041\n",
      "Epoch 90: Loss = 0.0045\n",
      "Epoch 91: Loss = 0.0037\n",
      "Epoch 92: Loss = 0.0037\n",
      "Epoch 93: Loss = 0.0040\n",
      "Epoch 94: Loss = 0.0020\n",
      "Epoch 95: Loss = 0.0023\n",
      "Epoch 96: Loss = 0.0021\n",
      "Epoch 97: Loss = 0.0020\n",
      "Epoch 98: Loss = 0.0036\n",
      "Epoch 99: Loss = 0.0083\n",
      "Epoch 100: Loss = 0.0043\n",
      "Epoch 101: Loss = 0.0028\n",
      "Epoch 102: Loss = 0.0070\n",
      "Epoch 103: Loss = 0.0033\n",
      "Epoch 104: Loss = 0.0014\n",
      "Epoch 105: Loss = 0.0013\n",
      "Epoch 106: Loss = 0.0064\n",
      "Epoch 107: Loss = 0.0031\n",
      "Epoch 108: Loss = 0.0012\n",
      "Epoch 109: Loss = 0.0010\n",
      "Epoch 110: Loss = 0.0011\n",
      "Epoch 111: Loss = 0.0064\n",
      "Epoch 112: Loss = 0.0018\n",
      "Epoch 113: Loss = 0.0034\n",
      "Epoch 114: Loss = 0.0030\n",
      "Epoch 115: Loss = 0.0032\n",
      "Epoch 116: Loss = 0.0047\n",
      "Epoch 117: Loss = 0.0019\n",
      "Epoch 118: Loss = 0.0018\n",
      "Epoch 119: Loss = 0.0054\n",
      "Epoch 120: Loss = 0.0034\n",
      "Epoch 121: Loss = 0.0006\n",
      "Epoch 122: Loss = 0.0042\n",
      "Epoch 123: Loss = 0.0008\n",
      "Epoch 124: Loss = 0.0018\n",
      "Epoch 125: Loss = 0.0007\n",
      "Epoch 126: Loss = 0.0102\n",
      "Epoch 127: Loss = 0.0013\n",
      "Epoch 128: Loss = 0.0011\n",
      "Epoch 129: Loss = 0.0043\n",
      "Epoch 130: Loss = 0.0012\n",
      "Epoch 131: Loss = 0.0011\n",
      "Epoch 132: Loss = 0.0011\n",
      "Epoch 133: Loss = 0.0040\n",
      "Epoch 134: Loss = 0.0037\n",
      "Epoch 135: Loss = 0.0006\n",
      "Epoch 136: Loss = 0.0007\n",
      "Epoch 137: Loss = 0.0021\n",
      "Epoch 138: Loss = 0.0007\n",
      "Epoch 139: Loss = 0.0006\n",
      "Epoch 140: Loss = 0.0053\n",
      "Epoch 141: Loss = 0.0021\n",
      "Epoch 142: Loss = 0.0012\n",
      "Epoch 143: Loss = 0.0004\n",
      "Epoch 144: Loss = 0.0012\n",
      "Epoch 145: Loss = 0.0014\n",
      "Epoch 146: Loss = 0.0004\n",
      "Epoch 147: Loss = 0.0011\n",
      "Epoch 148: Loss = 0.0015\n",
      "Epoch 149: Loss = 0.0003\n",
      "Epoch 150: Loss = 0.0008\n",
      "Epoch 151: Loss = 0.0011\n",
      "Epoch 152: Loss = 0.0014\n",
      "Epoch 153: Loss = 0.0031\n",
      "Epoch 154: Loss = 0.0052\n",
      "Epoch 155: Loss = 0.0009\n",
      "Epoch 156: Loss = 0.0006\n",
      "Epoch 157: Loss = 0.0021\n",
      "Epoch 158: Loss = 0.0001\n",
      "Epoch 159: Loss = 0.0003\n",
      "Epoch 160: Loss = 0.0011\n",
      "Epoch 161: Loss = 0.0002\n",
      "Epoch 162: Loss = 0.0006\n",
      "Epoch 163: Loss = 0.0011\n",
      "Epoch 164: Loss = 0.0004\n",
      "Epoch 165: Loss = 0.0012\n",
      "Epoch 166: Loss = 0.0013\n",
      "Epoch 167: Loss = 0.0013\n",
      "Epoch 168: Loss = 0.0006\n",
      "Epoch 169: Loss = 0.0008\n",
      "Epoch 170: Loss = 0.0007\n",
      "Epoch 171: Loss = 0.0010\n",
      "Epoch 172: Loss = 0.0005\n",
      "Epoch 173: Loss = 0.0002\n",
      "Epoch 174: Loss = 0.0023\n",
      "Epoch 175: Loss = 0.0010\n",
      "Epoch 176: Loss = 0.0003\n",
      "Epoch 177: Loss = 0.0016\n",
      "Epoch 178: Loss = 0.0013\n",
      "Epoch 179: Loss = 0.0002\n",
      "Epoch 180: Loss = 0.0001\n",
      "Epoch 181: Loss = 0.0020\n",
      "Epoch 182: Loss = 0.0020\n",
      "Epoch 183: Loss = 0.0055\n",
      "Epoch 184: Loss = 0.0007\n",
      "Epoch 185: Loss = 0.0002\n",
      "Epoch 186: Loss = 0.0001\n",
      "Epoch 187: Loss = 0.0002\n",
      "Epoch 188: Loss = 0.0038\n",
      "Epoch 189: Loss = 0.0001\n",
      "Epoch 190: Loss = 0.0004\n",
      "Epoch 191: Loss = 0.0032\n",
      "Epoch 192: Loss = 0.0017\n",
      "Epoch 193: Loss = 0.0001\n",
      "Epoch 194: Loss = 0.0004\n",
      "Epoch 195: Loss = 0.0021\n",
      "Epoch 196: Loss = 0.0002\n",
      "Epoch 197: Loss = 0.0020\n",
      "Epoch 198: Loss = 0.0001\n",
      "Epoch 199: Loss = 0.0007\n",
      "Epoch 200: Loss = 0.0055\n",
      "Epoch 201: Loss = 0.0004\n",
      "Epoch 202: Loss = 0.0001\n",
      "Epoch 203: Loss = 0.0003\n",
      "Epoch 204: Loss = 0.0003\n",
      "Epoch 205: Loss = 0.0006\n",
      "Epoch 206: Loss = 0.0030\n",
      "Epoch 207: Loss = 0.0001\n",
      "Epoch 208: Loss = 0.0028\n",
      "Epoch 209: Loss = 0.0010\n",
      "Epoch 210: Loss = 0.0003\n",
      "Epoch 211: Loss = 0.0043\n",
      "Epoch 212: Loss = 0.0029\n",
      "Epoch 213: Loss = 0.0008\n",
      "Epoch 214: Loss = 0.0005\n",
      "Epoch 215: Loss = 0.0057\n",
      "Epoch 216: Loss = 0.0005\n",
      "Epoch 217: Loss = 0.0004\n",
      "Epoch 218: Loss = 0.0016\n",
      "Epoch 219: Loss = 0.0010\n",
      "Epoch 220: Loss = 0.0003\n",
      "Epoch 221: Loss = 0.0021\n",
      "Epoch 222: Loss = 0.0012\n",
      "Epoch 223: Loss = 0.0005\n",
      "Epoch 224: Loss = 0.0011\n",
      "Epoch 225: Loss = 0.0043\n",
      "Epoch 226: Loss = 0.0010\n",
      "Epoch 227: Loss = 0.0003\n",
      "Epoch 228: Loss = 0.0005\n",
      "Epoch 229: Loss = 0.0038\n",
      "Epoch 230: Loss = 0.0005\n",
      "Epoch 231: Loss = 0.0049\n",
      "Epoch 232: Loss = 0.0003\n",
      "Epoch 233: Loss = 0.0020\n",
      "Epoch 234: Loss = 0.0021\n",
      "Epoch 235: Loss = 0.0003\n",
      "Epoch 236: Loss = 0.0006\n",
      "Epoch 237: Loss = 0.0008\n",
      "Epoch 238: Loss = 0.0008\n",
      "Epoch 239: Loss = 0.0013\n",
      "Epoch 240: Loss = 0.0030\n",
      "Epoch 241: Loss = 0.0003\n",
      "Epoch 242: Loss = 0.0002\n",
      "Epoch 243: Loss = 0.0004\n",
      "Epoch 244: Loss = 0.0002\n",
      "Epoch 245: Loss = 0.0012\n",
      "Epoch 246: Loss = 0.0008\n",
      "Epoch 247: Loss = 0.0007\n",
      "Epoch 248: Loss = 0.0008\n",
      "Epoch 249: Loss = 0.0003\n",
      "Epoch 250: Loss = 0.0019\n",
      "Epoch 251: Loss = 0.0002\n",
      "Epoch 252: Loss = 0.0026\n",
      "Epoch 253: Loss = 0.0008\n",
      "Epoch 254: Loss = 0.0011\n",
      "Epoch 255: Loss = 0.0008\n",
      "Epoch 256: Loss = 0.0019\n",
      "Epoch 257: Loss = 0.0000\n",
      "Epoch 258: Loss = 0.0001\n",
      "Epoch 259: Loss = 0.0044\n",
      "Epoch 260: Loss = 0.0011\n",
      "Epoch 261: Loss = 0.0001\n",
      "Epoch 262: Loss = 0.0003\n",
      "Epoch 263: Loss = 0.0022\n",
      "Epoch 264: Loss = 0.0022\n",
      "Epoch 265: Loss = 0.0010\n",
      "Epoch 266: Loss = 0.0051\n",
      "Epoch 267: Loss = 0.0004\n",
      "Epoch 268: Loss = 0.0016\n",
      "Epoch 269: Loss = 0.0004\n",
      "Epoch 270: Loss = 0.0004\n",
      "Epoch 271: Loss = 0.0003\n",
      "Epoch 272: Loss = 0.0012\n",
      "Epoch 273: Loss = 0.0003\n",
      "Epoch 274: Loss = 0.0003\n",
      "Epoch 275: Loss = 0.0006\n",
      "Epoch 276: Loss = 0.0008\n",
      "Epoch 277: Loss = 0.0003\n",
      "Epoch 278: Loss = 0.0002\n",
      "Epoch 279: Loss = 0.0002\n",
      "Epoch 280: Loss = 0.0005\n",
      "Epoch 281: Loss = 0.0002\n",
      "Epoch 282: Loss = 0.0001\n",
      "Epoch 283: Loss = 0.0014\n",
      "Epoch 284: Loss = 0.0001\n",
      "Epoch 285: Loss = 0.0028\n",
      "Epoch 286: Loss = 0.0004\n",
      "Epoch 287: Loss = 0.0004\n",
      "Epoch 288: Loss = 0.0001\n",
      "Epoch 289: Loss = 0.0021\n",
      "Epoch 290: Loss = 0.0003\n",
      "Epoch 291: Loss = 0.0009\n",
      "Epoch 292: Loss = 0.0006\n",
      "Epoch 293: Loss = 0.0019\n",
      "Epoch 294: Loss = 0.0033\n",
      "Epoch 295: Loss = 0.0016\n",
      "Epoch 296: Loss = 0.0002\n",
      "Epoch 297: Loss = 0.0002\n",
      "Epoch 298: Loss = 0.0016\n",
      "Epoch 299: Loss = 0.0001\n",
      "Epoch 300: Loss = 0.0005\n",
      "Autoencoder error threshold: 7.718261622358114e-05\n",
      "Threshold: 7.718261622358114e-05\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T19:49:21.609168Z",
     "start_time": "2025-05-14T19:49:19.717970Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== INSPECTING PARSED DATA ===\n",
      "Total records: 1000\n",
      "Records with 'NaN' as string: 40\n",
      "Value types distribution: {'str': 1000}\n",
      "\n",
      "Sample of str: {'name': 'node_systemd_unit_state', 'value': 'NaN', 'labels': {'instance': '10.71.0.59:9103', '__name__': 'node_systemd_unit_state', 'os_type': 'linux', 'vm': 'Lubuntu V2', 'name': 'sysstat-rotate.service', 'state': 'inactive', 'job': 'node', 'type': 'oneshot'}, 'timestamp': datetime.datetime(2025, 5, 14, 13, 36, 28, tzinfo=<UTC>)}\n",
      "\n",
      "=== PATCHING METRICS PROCESSOR ===\n",
      "Patched MetricsProcessor.structure_metrics to handle NaN values\n",
      "\n",
      "=== INSPECTING DATAFRAME ===\n",
      "DataFrame shape: (793, 6)\n",
      "NaN values per column:\n",
      "vm            0\n",
      "metric        0\n",
      "value        37\n",
      "state        61\n",
      "service      61\n",
      "timestamp     0\n",
      "dtype: int64\n",
      "\n",
      "=== FIXING DATAFRAME ===\n",
      "Replacing 37 NaN values in column 'value' with 0\n",
      "Replacing 61 NaN values in column 'state' with 'unknown'\n",
      "Replacing 61 NaN values in column 'service' with 'unknown'\n",
      "\n",
      "=== PATCHING FEATURE EXTRACTOR ===\n",
      "Patched FeatureExtractor.extract_features to handle NaN values\n",
      "\n",
      "=== GENERATING SNAPSHOTS AND FEATURES ===\n",
      "Generated 2 snapshots\n",
      "\n",
      "=== INSPECTING FEATURES ===\n",
      "Features shape: (2, 15)\n",
      "Total NaN values: 0\n",
      "Rows with at least one NaN: 0 out of 2\n",
      "\n",
      "=== NORMALIZING FEATURES ===\n",
      "Normalized features shape: (2, 15)\n",
      "NaN values after normalization: 0\n",
      "\n",
      "=== VALIDATING TENSOR ===\n",
      "Validation of features tensor:\n",
      "  Shape: torch.Size([2, 15])\n",
      "  NaN values: 0\n",
      "  Inf values: 0\n",
      "  Min value: -1.0\n",
      "  Max value: 1.0\n",
      "\n",
      "=== PATCHING ANOMALY DETECTOR ===\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'AnomalyDetector' has no attribute 'forward'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 241\u001B[39m\n\u001B[32m    236\u001B[39m         \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mSkipping model training due to invalid tensor data\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    238\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m df, features, tensor\n\u001B[32m--> \u001B[39m\u001B[32m241\u001B[39m df_fixed, features_fixed, tensor_fixed = \u001B[43mmain_debug_workflow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparsed\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 218\u001B[39m, in \u001B[36mmain_debug_workflow\u001B[39m\u001B[34m(raw, parsed, df)\u001B[39m\n\u001B[32m    216\u001B[39m \u001B[38;5;66;03m# Patch AnomalyDetector\u001B[39;00m\n\u001B[32m    217\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m=== PATCHING ANOMALY DETECTOR ===\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m218\u001B[39m \u001B[43mpatch_anomaly_detector\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    220\u001B[39m \u001B[38;5;66;03m# Create DataLoader\u001B[39;00m\n\u001B[32m    221\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdata\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DataLoader, TensorDataset\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 99\u001B[39m, in \u001B[36mpatch_anomaly_detector\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     96\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\n\u001B[32m     98\u001B[39m \u001B[38;5;66;03m# Store original forward method\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m99\u001B[39m original_forward = \u001B[43mAnomalyDetector\u001B[49m\u001B[43m.\u001B[49m\u001B[43mforward\u001B[49m\n\u001B[32m    101\u001B[39m \u001B[38;5;66;03m# Create a wrapper that validates inputs\u001B[39;00m\n\u001B[32m    102\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward_fixed\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[32m    103\u001B[39m     \u001B[38;5;66;03m# Check for NaN in input tensor\u001B[39;00m\n",
      "\u001B[31mAttributeError\u001B[39m: type object 'AnomalyDetector' has no attribute 'forward'"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. First, inspect your raw parsed data more thoroughly\n",
    "def inspect_parsed_data(parsed_data):\n",
    "    print(f\"Total records: {len(parsed_data)}\")\n",
    "\n",
    "    # Check for string 'NaN' vs actual np.nan values\n",
    "    string_nan_count = sum(1 for record in parsed_data if record.get('value') == 'NaN')\n",
    "\n",
    "    # Check data types of values\n",
    "    value_types = {}\n",
    "    for record in parsed_data:\n",
    "        value_type = type(record.get('value')).__name__\n",
    "        value_types[value_type] = value_types.get(value_type, 0) + 1\n",
    "\n",
    "    print(f\"Records with 'NaN' as string: {string_nan_count}\")\n",
    "    print(\"Value types distribution:\", value_types)\n",
    "\n",
    "    # Sample of each value type\n",
    "    for val_type in value_types:\n",
    "        sample = next((rec for rec in parsed_data if type(rec.get('value')).__name__ == val_type), None)\n",
    "        if sample:\n",
    "            print(f\"\\nSample of {val_type}:\", sample)\n",
    "\n",
    "# 2. Modify MetricsProcessor to handle NaN values properly\n",
    "def patch_structure_metrics():\n",
    "    from metrics_processor import MetricsProcessor\n",
    "    import numpy as np\n",
    "\n",
    "    # Store the original method\n",
    "    original_structure_metrics = MetricsProcessor.structure_metrics\n",
    "\n",
    "    # Create a wrapper that handles NaN values\n",
    "    def structure_metrics_fixed(data):\n",
    "        for item in data:\n",
    "            # Convert string 'NaN' to actual np.nan\n",
    "            if item.get('value') == 'NaN':\n",
    "                item['value'] = np.nan\n",
    "            # Try converting string numeric values to float\n",
    "            elif isinstance(item.get('value'), str):\n",
    "                try:\n",
    "                    item['value'] = float(item['value'])\n",
    "                except ValueError:\n",
    "                    # If conversion fails, set to np.nan\n",
    "                    item['value'] = np.nan\n",
    "\n",
    "        # Now call the original method\n",
    "        return original_structure_metrics(data)\n",
    "\n",
    "    # Replace the method\n",
    "    MetricsProcessor.structure_metrics = structure_metrics_fixed\n",
    "    print(\"Patched MetricsProcessor.structure_metrics to handle NaN values\")\n",
    "\n",
    "# 3. Add NaN handling to the feature extraction process\n",
    "def inspect_features(features):\n",
    "    import numpy as np\n",
    "\n",
    "    print(f\"Features shape: {features.shape}\")\n",
    "\n",
    "    # Check for NaN values\n",
    "    nan_count = np.isnan(features).sum()\n",
    "    nan_rows = np.isnan(features).any(axis=1).sum()\n",
    "\n",
    "    print(f\"Total NaN values: {nan_count}\")\n",
    "    print(f\"Rows with at least one NaN: {nan_rows} out of {features.shape[0]}\")\n",
    "\n",
    "    # Show distribution of NaN by column\n",
    "    if nan_count > 0:\n",
    "        nan_by_col = np.isnan(features).sum(axis=0)\n",
    "        print(\"NaN distribution by column:\")\n",
    "        for i, count in enumerate(nan_by_col):\n",
    "            if count > 0:\n",
    "                print(f\"  Column {i}: {count} NaN values\")\n",
    "\n",
    "# 4. Fix feature extraction to handle NaN values\n",
    "def patch_feature_extractor():\n",
    "    from feature_extractor import FeatureExtractor\n",
    "    import numpy as np\n",
    "\n",
    "    original_extract_features = FeatureExtractor.extract_features\n",
    "\n",
    "    def extract_features_fixed(snapshot):\n",
    "        # First get features using the original method\n",
    "        features = original_extract_features(snapshot)\n",
    "\n",
    "        # Replace any NaN values with appropriate defaults (0 or means)\n",
    "        features = np.nan_to_num(features, nan=0.0)\n",
    "\n",
    "        return features\n",
    "\n",
    "    # Replace the method\n",
    "    FeatureExtractor.extract_features = extract_features_fixed\n",
    "    print(\"Patched FeatureExtractor.extract_features to handle NaN values\")\n",
    "\n",
    "# 5. Modify the model to handle potential NaN inputs\n",
    "def patch_anomaly_detector():\n",
    "    from anomaly_detector import AnomalyDetector\n",
    "    import torch\n",
    "\n",
    "    # Store original forward method\n",
    "    original_forward = AnomalyDetector.forward\n",
    "\n",
    "    # Create a wrapper that validates inputs\n",
    "    def forward_fixed(self, x):\n",
    "        # Check for NaN in input tensor\n",
    "        if torch.isnan(x).any():\n",
    "            print(\"WARNING: NaN values detected in model input\")\n",
    "            # Replace NaN with zeros\n",
    "            x = torch.nan_to_num(x, nan=0.0)\n",
    "\n",
    "        return original_forward(self, x)\n",
    "\n",
    "    # Replace the method\n",
    "    AnomalyDetector.forward = forward_fixed\n",
    "    print(\"Patched AnomalyDetector.forward to handle NaN inputs\")\n",
    "\n",
    "# 6. Create a function to validate tensors before training\n",
    "def validate_tensor(tensor, name=\"tensor\"):\n",
    "    import torch\n",
    "\n",
    "    nan_count = torch.isnan(tensor).sum().item()\n",
    "    inf_count = torch.isinf(tensor).sum().item()\n",
    "\n",
    "    print(f\"Validation of {name}:\")\n",
    "    print(f\"  Shape: {tensor.shape}\")\n",
    "    print(f\"  NaN values: {nan_count}\")\n",
    "    print(f\"  Inf values: {inf_count}\")\n",
    "    print(f\"  Min value: {tensor.min().item()}\")\n",
    "    print(f\"  Max value: {tensor.max().item()}\")\n",
    "\n",
    "    return nan_count == 0 and inf_count == 0\n",
    "\n",
    "# 7. Main debugging workflow\n",
    "def main_debug_workflow(raw, parsed=None, df=None):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # If parsed data not provided, parse it\n",
    "    if parsed is None:\n",
    "        from data_parser import DataParser\n",
    "        parsed = DataParser.parse_minio_data(raw)\n",
    "\n",
    "    # Inspect parsed data\n",
    "    print(\"\\n=== INSPECTING PARSED DATA ===\")\n",
    "    inspect_parsed_data(parsed)\n",
    "\n",
    "    # Patch MetricsProcessor\n",
    "    print(\"\\n=== PATCHING METRICS PROCESSOR ===\")\n",
    "    patch_structure_metrics()\n",
    "\n",
    "    # Apply MetricsProcessor if df not provided\n",
    "    if df is None:\n",
    "        from metrics_processor import MetricsProcessor\n",
    "        critical = MetricsProcessor.filter_critical_data(parsed)\n",
    "        structured = MetricsProcessor.structure_metrics(critical)\n",
    "        df = pd.DataFrame(structured)\n",
    "\n",
    "    # Check DataFrame for NaN values\n",
    "    print(\"\\n=== INSPECTING DATAFRAME ===\")\n",
    "    print(f\"DataFrame shape: {df.shape}\")\n",
    "    print(f\"NaN values per column:\")\n",
    "    print(df.isna().sum())\n",
    "\n",
    "    # Fill NaN values in the DataFrame\n",
    "    print(\"\\n=== FIXING DATAFRAME ===\")\n",
    "    # For numeric columns, replace NaN with 0\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    for col in numeric_cols:\n",
    "        nan_count = df[col].isna().sum()\n",
    "        if nan_count > 0:\n",
    "            print(f\"Replacing {nan_count} NaN values in column '{col}' with 0\")\n",
    "            df[col] = df[col].fillna(0)\n",
    "\n",
    "    # For non-numeric columns, replace NaN with appropriate defaults\n",
    "    cat_cols = df.select_dtypes(exclude=['number']).columns\n",
    "    for col in cat_cols:\n",
    "        nan_count = df[col].isna().sum()\n",
    "        if nan_count > 0:\n",
    "            default = \"unknown\" if col != 'timestamp' else pd.Timestamp.now()\n",
    "            print(f\"Replacing {nan_count} NaN values in column '{col}' with '{default}'\")\n",
    "            df[col] = df[col].fillna(default)\n",
    "\n",
    "    # Patch FeatureExtractor\n",
    "    print(\"\\n=== PATCHING FEATURE EXTRACTOR ===\")\n",
    "    patch_feature_extractor()\n",
    "\n",
    "    # Generate snapshots and extract features\n",
    "    print(\"\\n=== GENERATING SNAPSHOTS AND FEATURES ===\")\n",
    "    from snapshot_generator import SnapshotGenerator\n",
    "    snap_gen = SnapshotGenerator()\n",
    "    snapshots, per_vm_prompts, multi_vm = snap_gen.generate_prompts_from_df(df, window_minutes=5)\n",
    "    print(f\"Generated {len(snapshots)} snapshots\")\n",
    "\n",
    "    # Extract and inspect features\n",
    "    from feature_extractor import FeatureExtractor\n",
    "    features = np.vstack([FeatureExtractor.extract_features(s) for s in snapshots])\n",
    "    print(\"\\n=== INSPECTING FEATURES ===\")\n",
    "    inspect_features(features)\n",
    "\n",
    "    # Normalize features\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    print(\"\\n=== NORMALIZING FEATURES ===\")\n",
    "    # Handle potential NaN values before scaling\n",
    "    features = np.nan_to_num(features, nan=0.0)\n",
    "    scaler = StandardScaler().fit(features)\n",
    "    fn = scaler.transform(features)\n",
    "    print(f\"Normalized features shape: {fn.shape}\")\n",
    "    # Check for NaN after normalization\n",
    "    nan_after_norm = np.isnan(fn).sum()\n",
    "    print(f\"NaN values after normalization: {nan_after_norm}\")\n",
    "\n",
    "    # Convert to tensor\n",
    "    import torch\n",
    "    tensor = torch.tensor(fn, dtype=torch.float32)\n",
    "    print(\"\\n=== VALIDATING TENSOR ===\")\n",
    "    is_valid = validate_tensor(tensor, \"features tensor\")\n",
    "\n",
    "    # Patch AnomalyDetector\n",
    "    print(\"\\n=== PATCHING ANOMALY DETECTOR ===\")\n",
    "    patch_anomaly_detector()\n",
    "\n",
    "    # Create DataLoader\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    dl = DataLoader(TensorDataset(tensor), batch_size=2, shuffle=True)\n",
    "\n",
    "    # Train model if tensor is valid\n",
    "    if is_valid:\n",
    "        print(\"\\n=== TRAINING MODEL ===\")\n",
    "        from anomaly_detector import AnomalyDetector\n",
    "        model = AnomalyDetector(input_dim=tensor.shape[1], latent_dim=15, dropout_rate=0.2)\n",
    "        model.train_autoencoder(dl, num_epochs=10, lr=1e-3, weight_decay=1e-5)  # Reduced epochs for testing\n",
    "\n",
    "        # Compute threshold\n",
    "        print(\"\\n=== COMPUTING THRESHOLD ===\")\n",
    "        model.compute_threshold(tensor)\n",
    "        print(\"Threshold:\", model.threshold.item())\n",
    "    else:\n",
    "        print(\"\\nSkipping model training due to invalid tensor data\")\n",
    "\n",
    "    return df, features, tensor\n",
    "\n",
    "\n",
    "df_fixed, features_fixed, tensor_fixed = main_debug_workflow(raw, parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm-pipeline)",
   "language": "python",
   "name": "llm-pipeline"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
